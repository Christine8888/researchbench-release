{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResearchBench Evaluation Analysis\n",
    "\n",
    "This notebook demonstrates how to load and analyze Inspect AI evaluation logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(src_path) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys.path:\n\u001b[32m     10\u001b[39m     sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(src_path))\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataset\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataloader\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexperiments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manalysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     load_eval_logs_to_dataframe,\n\u001b[32m     15\u001b[39m     aggregate_runs,\n\u001b[32m     16\u001b[39m     get_model_summary_stats,\n\u001b[32m     17\u001b[39m     get_per_paper_stats\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m pd.set_option(\u001b[33m'\u001b[39m\u001b[33mdisplay.max_columns\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "src_path = Path.cwd().parent.parent\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "from dataset.dataloader import Dataloader\n",
    "from experiments.utils.analysis import (\n",
    "    load_eval_logs_to_dataframe,\n",
    "    aggregate_runs,\n",
    "    get_model_summary_stats,\n",
    "    get_per_paper_stats\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(f\"Working from: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Paths\n",
    "\n",
    "Update these paths to point to your evaluation logs and data directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dirs = [\n",
    "    \"logs/model1-run-1\",\n",
    "    \"logs/model1-run-2\",\n",
    "    \"logs/model1-run-3\",\n",
    "    \"logs/model2-run-1\",\n",
    "    \"logs/model2-run-2\",\n",
    "    \"logs/model2-run-3\",\n",
    "]\n",
    "\n",
    "existing_dirs = [d for d in log_dirs if Path(d).exists() or Path(f\"../../{d}\").exists()]\n",
    "existing_dirs = [str(Path(f\"../../{d}\").resolve()) if not Path(d).exists() else d for d in existing_dirs]\n",
    "\n",
    "print(f\"Found {len(existing_dirs)} log directories\")\n",
    "for d in existing_dirs:\n",
    "    print(f\"  - {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataloader\n",
    "\n",
    "Load papers and tasks for computing difficulty-weighted scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = Dataloader(\n",
    "    task_types=[\"numeric\", \"code\"],\n",
    "    load_text=False\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dataloader.papers)} papers\")\n",
    "print(f\"Total tasks: {sum(len(p.tasks) for p in dataloader.papers.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Evaluation Logs\n",
    "\n",
    "Load all evaluation logs into a structured DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not existing_dirs:\n",
    "    print(\"WARNING: No log directories found. Creating empty DataFrame.\")\n",
    "    df = pd.DataFrame()\n",
    "else:\n",
    "    df = load_eval_logs_to_dataframe(existing_dirs, dataloader)\n",
    "    print(f\"\\nLoaded {len(df)} rows\")\n",
    "    print(f\"Models: {df.index.get_level_values('model').unique().tolist()}\")\n",
    "    print(f\"Papers: {len(df.index.get_level_values('paper').unique())} unique papers\")\n",
    "    print(f\"\\nDataFrame structure:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Summary Statistics\n",
    "\n",
    "Overall performance across all papers and runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    model_stats = get_model_summary_stats(df)\n",
    "    print(\"\\n=== MODEL SUMMARY STATISTICS ===\")\n",
    "    display(model_stats.round(3))\n",
    "else:\n",
    "    print(\"No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Per-Paper Statistics\n",
    "\n",
    "Performance breakdown by paper across all models and runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    paper_stats = get_per_paper_stats(df)\n",
    "    print(\"\\n=== PER-PAPER STATISTICS ===\")\n",
    "    display(paper_stats.round(3))\n",
    "else:\n",
    "    print(\"No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Accuracy by Model and Paper\n",
    "\n",
    "Average accuracy for each model on each paper (aggregated across runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    accuracy_table = aggregate_runs(df, \"accuracy\", \"mean\")\n",
    "    print(\"\\n=== ACCURACY BY MODEL AND PAPER (Mean across runs) ===\")\n",
    "    display(accuracy_table.round(3))\n",
    "else:\n",
    "    print(\"No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Difficulty-Weighted Accuracy\n",
    "\n",
    "Difficulty-weighted scores accounting for task complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    difficulty_weighted_table = aggregate_runs(df, \"difficulty_weighted_accuracy\", \"mean\")\n",
    "    print(\"\\n=== DIFFICULTY-WEIGHTED ACCURACY BY MODEL AND PAPER ===\")\n",
    "    display(difficulty_weighted_table.round(3))\n",
    "else:\n",
    "    print(\"No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Run Performance\n",
    "\n",
    "Best accuracy achieved by each model on each paper (max across runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    best_accuracy_table = aggregate_runs(df, \"accuracy\", \"max\")\n",
    "    print(\"\\n=== BEST ACCURACY BY MODEL AND PAPER (Max across runs) ===\")\n",
    "    display(best_accuracy_table.round(3))\n",
    "else:\n",
    "    print(\"No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Token Usage Statistics\n",
    "\n",
    "Average token usage by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    summary_df = df[df.index.get_level_values(\"task\") == \"_summary\"].copy()\n",
    "    \n",
    "    token_stats = summary_df.groupby(\"model\").agg({\n",
    "        \"input_tokens\": \"mean\",\n",
    "        \"output_tokens\": \"mean\",\n",
    "        \"reasoning_tokens\": \"mean\",\n",
    "        \"runtime_minutes\": \"mean\"\n",
    "    }).round(0)\n",
    "    \n",
    "    print(\"\\n=== TOKEN USAGE BY MODEL ===\")\n",
    "    display(token_stats)\n",
    "else:\n",
    "    print(\"No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Task-Level Analysis\n",
    "\n",
    "Performance on individual tasks across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    task_df = df[df.index.get_level_values(\"task\") != \"_summary\"].copy()\n",
    "    \n",
    "    if not task_df.empty:\n",
    "        task_stats = task_df.groupby([\"paper\", \"task\"]).agg({\n",
    "            \"task_score\": [\"mean\", \"std\", \"count\"],\n",
    "            \"task_difficulty\": \"first\"\n",
    "        }).round(3)\n",
    "        \n",
    "        print(\"\\n=== TASK-LEVEL STATISTICS (First 20 tasks) ===\")\n",
    "        display(task_stats.head(20))\n",
    "    else:\n",
    "        print(\"No task-level data available\")\n",
    "else:\n",
    "    print(\"No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Tables\n",
    "\n",
    "Save tables to CSV files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    output_dir = Path(\"table_outputs\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    model_stats.to_csv(output_dir / \"model_summary.csv\")\n",
    "    paper_stats.to_csv(output_dir / \"paper_summary.csv\")\n",
    "    accuracy_table.to_csv(output_dir / \"accuracy_by_model_paper.csv\")\n",
    "    difficulty_weighted_table.to_csv(output_dir / \"difficulty_weighted_accuracy.csv\")\n",
    "    \n",
    "    print(f\"\\nTables exported to {output_dir}/\")\n",
    "    print(\"  - model_summary.csv\")\n",
    "    print(\"  - paper_summary.csv\")\n",
    "    print(\"  - accuracy_by_model_paper.csv\")\n",
    "    print(\"  - difficulty_weighted_accuracy.csv\")\n",
    "else:\n",
    "    print(\"No data to export\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
